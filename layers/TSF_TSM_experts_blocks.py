import torch
import torch.nn as nn
import torch.nn.functional as F
from .TSF_TSM_layers import PatchingEmbedding
from .SelfAttention_Family import AttentionPool
import math

class ContextualCouplingTSA(nn.Module):
    """
    Affine Coupling êµ¬ì¡°ë¥¼ ì»¨í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì— ì ìš©í•œ TSA.
    
    ì…ë ¥ì˜ ì ˆë°˜(x1)ìœ¼ë¡œ ì–´í…ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³ , 
    ì´ë¥¼ ì´ìš©í•´ ë‹¤ë¥¸ ì ˆë°˜(x2)ì„ ë³€í˜•ì‹œí‚¬ íŒŒë¼ë¯¸í„°(s, t)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        # ì•„í•€ ì»¤í”Œë§ì„ ìœ„í•´ d_modelì€ ì§ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
        assert d_model % 2 == 0, "d_model must be an even number."
        
        self.d_model = d_model
        self.d_channel = d_model // 2 # ì…ë ¥ì„ ë‘˜ë¡œ ë‚˜ëˆ„ë¯€ë¡œ ì‹¤ì œ ì²˜ë¦¬ ì±„ë„ ìˆ˜ëŠ” ì ˆë°˜
        self.n_heads = n_heads
        self.d_head = self.d_channel // n_heads

        # ğŸ’¡ ë³€ê²½ í¬ì¸íŠ¸: Q, KëŠ” d_channelì„ ê¸°ì¤€ìœ¼ë¡œ ìƒì„± (VëŠ” ì‚¬ìš© ì•ˆ í•¨)
        self.query_projection = nn.Linear(self.d_channel, self.d_channel)
        self.key_projection = nn.Linear(self.d_channel, self.d_channel)
        
        # Saliency ê³„ì‚°ì„ ìœ„í•œ Conv1d (ì…ë ¥ ì±„ë„ë„ ì ˆë°˜)
        self.saliency_conv = nn.Conv1d(
            in_channels=self.d_channel,
            out_channels=n_heads,
            kernel_size=3,
            padding='same'
        )

        # ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸: ì–´í…ì…˜ ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° scale(s)ê³¼ shift(t)ë¥¼ ìƒì„±í•˜ëŠ” ë ˆì´ì–´
        # sì™€ t ëª¨ë‘ d_channel í¬ê¸°ë¥¼ ê°€ì§€ë¯€ë¡œ ì¶œë ¥ì€ d_channel * 2
        self.st_projection = nn.Linear(self.d_channel, self.d_channel)
        
        # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ (ì›ë³¸ TSAì™€ ë™ì¼)
        self.out_projection = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, attn_mask=None):
        # src shape: [B, L, d_model]
        
        # 1. ğŸ’¡ ì…ë ¥ì„ x1, x2ë¡œ ë¶„í• 
        x1, x2 = src.chunk(2, dim=-1) # [B, L, d_channel]
        
        B, L, _ = x1.shape

        # 2. x1ì„ ì´ìš©í•´ Q, K, Saliency ìƒì„± (ì›ë³¸ TSAì™€ ê±°ì˜ ë™ì¼)
        Q = self.query_projection(x1).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        K = self.key_projection(x1).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        
        saliency_scores = self.saliency_conv(x1.transpose(1, 2)).unsqueeze(2)

        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        attention_scores = attention_scores + saliency_scores

        if attn_mask is not None:
            attention_scores = attention_scores.masked_fill(attn_mask == 0, -1e9)

        # 3. ğŸ’¡ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ìš©í•´ ì»¨í…ìŠ¤íŠ¸ ë²¡í„° ìƒì„± (V ëŒ€ì‹  K ì‚¬ìš©)
        # ì¸ì½”ë”ì´ë¯€ë¡œ softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Kë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        context = torch.matmul(attention_weights, K) # [B, n_heads, L, d_head]
        context = context.transpose(1, 2).contiguous().view(B, L, self.d_channel)

        # 4. ğŸ’¡ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¡œë¶€í„° ê²Œì´íŠ¸ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
        g_params = self.st_projection(context) 
        gate = torch.sigmoid(g_params)
        
        # 5. x2ì— ê²Œì´íŠ¸ ë³€í™˜ ì ìš©
        transformed_x2 = torch.tanh(x2) 
        y2 = gate * transformed_x2 + (1 - gate) * x2

        # 6. ğŸ’¡ ë³€í™˜ë˜ì§€ ì•Šì€ x1ê³¼ ë³€í™˜ëœ y2ë¥¼ í•©ì¹˜ê³  ìµœì¢… ì¶œë ¥
        output = torch.cat([x1, y2], dim=-1) # [B, L, d_model]
        output = self.out_projection(output)

        return output, attention_weights # MoE ë ˆì´ì–´ì™€ í˜¸í™˜ë˜ëŠ” ì¶œë ¥ í˜•íƒœ

class TSA(nn.Module):
    """
    Temporal Saliency Attention (TSA)
    ì‹œê³„ì—´ì˜ ê° ì‹œì (íŒ¨ì¹˜)ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì–´í…ì…˜ ìŠ¤ì½”ì–´ì— ë°˜ì˜í•©ë‹ˆë‹¤.
    """
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        # Q, K, Vë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.query_projection = nn.Linear(d_model, d_model)
        self.key_projection = nn.Linear(d_model, d_model)
        self.value_projection = nn.Linear(d_model, d_model)
        
        # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.out_projection = nn.Linear(d_model, d_model)
        
        # ì‹œê°„ì  ì¤‘ìš”ë„(Saliency)ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ 1D Conv ë ˆì´ì–´
        self.saliency_conv = nn.Conv1d(
            in_channels=d_model, 
            out_channels=n_heads, # ê° í—¤ë“œë³„ë¡œ ë‹¤ë¥¸ saliencyë¥¼ í•™ìŠµ
            kernel_size=3, 
            padding='same'
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, src, attn_mask=None):
        # src shape: [Batch, Seq_Len, d_model]
        B, L, _ = src.shape

        # 1. Q, K, V ìƒì„± ë° n_headsë¡œ ë¶„í• 
        Q = self.query_projection(src).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        K = self.key_projection(src).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        V = self.value_projection(src).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        # Q, K, V shape: [B, n_heads, Seq_Len, d_head]

        # 2. ì‹œê°„ì  ì¤‘ìš”ë„(Saliency) ìŠ¤ì½”ì–´ ê³„ì‚°
        # src: [B, L, C] -> [B, C, L] for Conv1d
        saliency_scores = self.saliency_conv(src.transpose(1, 2)) # -> [B, n_heads, L]
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ì™€ ë§ì…ˆì„ ìœ„í•´ ì°¨ì› í™•ì¥: [B, n_heads, 1, L]
        saliency_scores = saliency_scores.unsqueeze(2)

        # 3. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° ë° Saliency ë°˜ì˜
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        
        # ğŸ’¡ í•µì‹¬: Softmax ì´ì „ì— Saliency ìŠ¤ì½”ì–´ë¥¼ ë”í•´ì¤Œ
        # Saliency ìŠ¤ì½”ì–´ê°€ ë¸Œë¡œë“œìºìŠ¤íŒ…ë˜ì–´ ëª¨ë“  Queryì— ëŒ€í•œ Keyì˜ ì¤‘ìš”ë„ë¡œ ë”í•´ì§
        attention_scores = attention_scores + saliency_scores

        if attn_mask is not None:
            attention_scores = attention_scores.masked_fill(attn_mask == 0, -1e9)

        # 4. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° Vì™€ ê²°í•©
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, V) # [B, n_heads, L, d_head]

        # 5. í—¤ë“œë“¤ì„ ë‹¤ì‹œ í•©ì¹˜ê³  ìµœì¢… ì¶œë ¥
        output = output.transpose(1, 2).contiguous().view(B, L, self.d_model)
        output = self.out_projection(output)

        return output, attention_weights # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë„ ë°˜í™˜í•˜ì—¬ ë¶„ì„ì— í™œìš© ê°€ëŠ¥    

class APGELU(nn.Module):
    """
    Adaptive Phase GELU (AP-GELU) - ìˆ˜ì •ëœ ë²„ì „
    - 2D ì…ë ¥ [Tokens, Features]ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì •
    """
    def __init__(self, feature_dim): # d_model ëŒ€ì‹  feature_dimìœ¼ë¡œ ì´ë¦„ ë³€ê²½
        super().__init__()
        self.omega_head = nn.Linear(feature_dim, feature_dim)
        self.phi_head = nn.Linear(feature_dim, feature_dim)

        nn.init.constant_(self.omega_head.weight, 0)
        nn.init.constant_(self.omega_head.bias, 1)
        nn.init.constant_(self.phi_head.weight, 0)
        nn.init.constant_(self.phi_head.bias, 0)

    def forward(self, x):
        # x shape: [Tokens, Features] (ì˜ˆ: [4967, 256])
        
        # ğŸ’¡ í•µì‹¬: í† í° ì°¨ì›(dim=0)ì— ëŒ€í•´ í‰ê· ì„ ë‚´ì–´ í†µê³„ì¹˜ ê³„ì‚°
        # ì´ë ‡ê²Œ í•˜ë©´ í˜„ì¬ ì „ë¬¸ê°€ì—ê²Œ ë“¤ì–´ì˜¨ ëª¨ë“  í† í°ë“¤ì˜ í‰ê· ì  íŠ¹ì„±ì„ ë°˜ì˜
        if x.dim() < 2 or x.shape[0] <= 1: # í† í°ì´ í•˜ë‚˜ì´ê±°ë‚˜ ì—†ì„ ê²½ìš°
             return F.gelu(x)

        token_stats = x.mean(dim=0) # -> [Features]

        # í†µê³„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ omegaì™€ phi ì˜ˆì¸¡
        omega = F.softplus(self.omega_head(token_stats)) # -> [Features]
        phi = self.phi_head(token_stats) # -> [Features]
        
        # omegaì™€ phiê°€ [Features] í˜•íƒœì´ë¯€ë¡œ [Tokens, Features] í˜•íƒœì¸ xì— ë¸Œë¡œë“œìºìŠ¤íŒ…ë˜ì–´ ì—°ì‚°ë¨
        transformed_x = omega * x + phi
        
        return F.gelu(transformed_x)

class MoE(nn.Module):
    """Mixture of Experts FFN ë ˆì´ì–´."""
    def __init__(self, d_model, d_ff, num_experts, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_ff),
                APGELU(d_ff),
                nn.Linear(d_ff, d_model)
            ) for _ in range(num_experts)
        ])
        self.gating_network = nn.Linear(d_model, num_experts)

    def forward(self, x):
        # x shape: [B, L, C]
        logits = self.gating_network(x) # [B, L, num_experts]
        gates = F.softmax(logits, dim=-1)
        
        # ê° í† í°ì— ëŒ€í•´ top-k ì „ë¬¸ê°€ ì„ íƒ
        top_k_gates, top_k_indices = gates.topk(self.top_k, dim=-1) # [B, L, top_k]
        
        # top-k ê°€ì¤‘ì¹˜ ì •ê·œí™”
        top_k_gates = top_k_gates / top_k_gates.sum(dim=-1, keepdim=True)
        
        # ì „ë¬¸ê°€ ê³„ì‚°
        output = torch.zeros_like(x)
        flat_x = x.view(-1, x.size(-1))
        flat_indices = top_k_indices.view(-1, self.top_k)

        for i in range(self.num_experts):
            # ië²ˆì§¸ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•œ í† í°ë“¤ì˜ ë§ˆìŠ¤í¬ ìƒì„±
            mask, _ = (flat_indices == i).max(dim=-1)
            if mask.any():
                expert_input = flat_x[mask]
                expert_output = self.experts[i](expert_input)
                
                # ê°€ì¤‘í•©ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
                gate_values = top_k_gates.view(-1, self.top_k)[mask]
                indices_values = top_k_indices.view(-1, self.top_k)[mask]
                expert_gate = gate_values[indices_values == i]
                
                # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ê²°ê³¼ ì €ì¥
                output.view(-1, x.size(-1))[mask] += expert_output * expert_gate.unsqueeze(-1)
        return output

class MoETSAEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, num_experts, dropout=0.1):
        super().__init__()
        # self.self_attn = TSA(d_model, n_heads, dropout=dropout)
        self.self_attn = ContextualCouplingTSA(d_model, n_heads, dropout=dropout)
        self.moe_ffn = MoE(d_model, d_ff, num_experts)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, attn_mask=None):
        src2, attn_weights = self.self_attn(src, attn_mask=attn_mask)
        src = src + self.dropout(src2)
        src = self.norm1(src)
        src2 = self.moe_ffn(src)
        src = src + self.dropout(src2)
        src = self.norm2(src)
        return src, attn_weights

class ContextEncoder(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.patching_embedding = PatchingEmbedding(configs.patch_len, configs.stride, configs.enc_in, configs.d_model)
        self.pos_encoder = nn.Parameter(torch.randn(1, (configs.seq_len - configs.patch_len) // configs.stride + 1, configs.d_model))
        self.encoder_layers = nn.ModuleList([
            MoETSAEncoderLayer(configs.d_model, configs.n_heads, configs.d_ff, configs.num_experts, configs.dropout)
            for _ in range(configs.e_layers)
        ])
        self.attention_pool = AttentionPool(d_model=configs.d_model, n_heads=configs.n_heads)

    def forward(self, x, get_attn=False):
        x_patched = self.patching_embedding(x)
        x_patched += self.pos_encoder

        attn_weights_list = []
        
        for layer in self.encoder_layers:
            x_patched, attn_weights  = layer(x_patched)
            if get_attn:
                attn_weights_list.append(attn_weights)
        
        summary_context = self.attention_pool(x_patched)

        if get_attn:
            return summary_context, attn_weights_list
        
        return summary_context