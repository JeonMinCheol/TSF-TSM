import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import logging

# nflows ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from nflows.flows.base import Flow
from nflows.distributions.normal import StandardNormal
from nflows.transforms.base import CompositeTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform
from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform
from nflows.transforms.permutations import RandomPermutation
from nflows.transforms.normalization import BatchNorm
from nflows.nn.nets import ResidualNet

def create_conditional_nsf_flow(feature_dim, context_dim, num_layers=5, hidden_features=128, num_bins=8):
    transforms = []
    mask = torch.zeros(feature_dim)
    mask[:feature_dim // 2] = 1

    for i in range(num_layers):
        transforms.append(RandomPermutation(features=feature_dim))
        current_mask = mask if i % 2 == 0 else 1 - mask
        transforms.append(
            PiecewiseRationalQuadraticCouplingTransform(
                mask=current_mask,
                transform_net_create_fn=lambda in_features, out_features: ResidualNet(
                    in_features=in_features,
                    out_features=out_features,
                    hidden_features=hidden_features,
                    context_features=context_dim,
                    num_blocks=2,
                    activation=F.relu,
                ),
                num_bins=num_bins, tails="linear", tail_bound=5.0
            )
        )
        transforms.append(BatchNorm(features=feature_dim))

    return Flow(
        transform=CompositeTransform(transforms),
        distribution=StandardNormal([feature_dim])
    )


class PatchingEmbedding(nn.Module):
    def __init__(self, patch_len, stride, c_in, d_model):
        super().__init__()
        self.patch_len = patch_len
        self.stride = stride
        
        # ê° íŒ¨ì¹˜ë¥¼ d_model ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•  Linear ë ˆì´ì–´
        # íŒ¨ì¹˜ëŠ” (patch_len * c_in) í¬ê¸°ì˜ 1D ë²¡í„°ë¡œ í¼ì³ì§‘ë‹ˆë‹¤.
        self.projection = nn.Linear(patch_len * c_in, d_model)

    def forward(self, x):
        # x shape: [Batch, Seq_Len, Features]
        # ì˜ˆ: [512, 336, 7]
        
        # 1. ì‹œí€€ìŠ¤ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ìë¦…ë‹ˆë‹¤. (stride ë§Œí¼ ê²¹ì¹˜ë©´ì„œ)
        # unfold: í…ì„œë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ë¼ì£¼ëŠ” ë§¤ìš° íš¨ìœ¨ì ì¸ í•¨ìˆ˜
        # (B, L, C) -> (B, C, L) -> unfold -> (B, C, Num_Patches, Patch_Len)
        x_unfolded = x.permute(0, 2, 1).unfold(dimension=-1, size=self.patch_len, step=self.stride)
        
        # 2. íŒ¨ì¹˜ë¥¼ í¼ì¹˜ê¸°(flatten) ìœ„í•´ ì°¨ì›ì„ ì¬ì •ë ¬í•˜ê³  í•©ì¹©ë‹ˆë‹¤.
        # (B, C, Num_Patches, Patch_Len) -> (B, Num_Patches, C, Patch_Len)
        x_patched = x_unfolded.permute(0, 2, 1, 3)
        
        # (B, Num_Patches, C, Patch_Len) -> (B, Num_Patches, C * Patch_Len)
        B, n_patches, C, P = x_patched.shape
        x_flattened = x_patched.reshape(B, n_patches, -1)
        
        # 3. Linear ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ê° íŒ¨ì¹˜ë¥¼ d_model ì°¨ì›ì˜ ë²¡í„°ë¡œ ì„ë² ë”©
        # (B, Num_Patches, C * Patch_Len) -> (B, Num_Patches, d_model)
        out = self.projection(x_flattened)
        
        return out
        
"""
Summary context -> predicted correction sequence for residuals.
We subtract this from residuals before feeding flow (so flow models the 'whitened' residual).
During sampling we add this AR prediction back to the sampled residuals.
"""
class ResidualAR(nn.Module):
    def __init__(self, context_dim, pred_len, enc_in, hidden=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(context_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden//2),
            nn.ReLU(),
            nn.Linear(hidden//2, pred_len * enc_in)
        )
    def forward(self, context):
        # context: (B, context_dim) -> output (B, pred_len)
        return self.net(context)


"""
MLPë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì—ì„œ ë¹„ì„ í˜• ì¶”ì„¸ë¥¼ í•™ìŠµí•˜ê³  ë¶„ë¦¬í•˜ëŠ” ëª¨ë“ˆ
"""
class LearnableTrend(nn.Module):
    def __init__(self, seq_len, pred_len, enc_in):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        
        # ì‹œê°„ ì¸ë±ìŠ¤ë¥¼ ì…ë ¥ë°›ì•„ ì¶”ì„¸ ê°’ì„ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ MLP
        self.trend_mlp = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, enc_in)
        )

    def forward(self, x_batch):
        # x_batch shape: (B, current_seq_len, enc_in)
        
        # ğŸ’¡ [ìˆ˜ì •] ë°°ì¹˜ í¬ê¸°(B)ë¥¼ ì…ë ¥ í…ì„œë¡œë¶€í„° ê°€ì ¸ì˜µë‹ˆë‹¤.
        B, current_seq_len, _ = x_batch.shape
        
        # 2. í˜„ì¬ ê¸¸ì´ì— ë§ëŠ” ì‹œê°„ ì¸ë±ìŠ¤ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        past_time_idx = torch.arange(current_seq_len, device=x_batch.device, dtype=torch.float32)
        past_time_idx_normalized = (past_time_idx / (self.seq_len - 1)).unsqueeze(-1)

        # 3. MLPë¥¼ í†µê³¼ì‹œì¼œ ê³¼ê±° ì¶”ì„¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        #    - unsqueeze(0)ë¥¼ í†µí•´ ë¸Œë¡œë“œìºìŠ¤íŒ…ì´ ê°€ëŠ¥í•˜ë„ë¡ ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        past_trend = self.trend_mlp(past_time_idx_normalized).unsqueeze(0)
        
        # 4. ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì„¸ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        detrended_x = x_batch - past_trend
        
        # 5. ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œì ì— ëŒ€í•œ ì¶”ì„¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        #    - `past_time_idx` ëŒ€ì‹  `current_seq_len`ì„ ì‚¬ìš©í•˜ë©´ ë” ëª…í™•í•©ë‹ˆë‹¤.
        future_time_idx = torch.arange(self.seq_len, self.seq_len + self.pred_len, device=x_batch.device, dtype=torch.float32)
        future_time_idx_normalized = (future_time_idx / (self.seq_len - 1)).unsqueeze(-1)
        
        future_trend = self.trend_mlp(future_time_idx_normalized)

        # ğŸ’¡ [ìˆ˜ì •] future_trendë¥¼ ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ expand í•´ì¤ë‹ˆë‹¤.
        #    - (pred_len, enc_in) -> (1, pred_len, enc_in) -> (B, pred_len, enc_in)
        future_trend = future_trend.unsqueeze(0).expand(B, -1, -1)
        
        return detrended_x, future_trend

class DecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):
        super().__init__()
        d_ff = d_ff or 4 * d_model
        # 1. Masked Self-Attention (ë””ì½”ë”ê°€ ë¯¸ë˜ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•¨)
        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)
        # 2. Cross-Attention (ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ì°¸ê³ í•¨)
        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)
        # 3. Feed-Forward Network
        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff),
                                 nn.ReLU(),
                                 nn.Dropout(dropout),
                                 nn.Linear(d_ff, d_model))
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, memory, tgt_mask=None):
        # x: ë””ì½”ë” ì…ë ¥ [B, T_tgt, C]
        # memory: ì¸ì½”ë” ì¶œë ¥ [B, T_src, C]
        
        # 1. Masked Self-Attention
        x_attn, _ = self.self_attention(x, x, x, attn_mask=tgt_mask)
        x = self.norm1(x + self.dropout(x_attn))
        
        # 2. Cross-Attention
        x_cross_attn, _ = self.cross_attention(query=x, key=memory, value=memory)
        x = self.norm2(x + self.dropout(x_cross_attn))
        
        # 3. Feed-Forward
        x_ffn = self.ffn(x)
        x = self.norm3(x + self.dropout(x_ffn))
        
        return x

class SEBlock(nn.Module):
    """Squeeze-and-Excitation ë¸”ë¡. ì±„ë„(í”¼ì²˜)ë³„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."""
    def __init__(self, in_channels, reduction_ratio=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool1d(1)
        self.excitation = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction_ratio, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction_ratio, in_channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x shape: [Batch, Channels, Seq_Len]
        B, C, _ = x.shape
        y = self.squeeze(x).view(B, C) # Squeeze: [B, C]
        y = self.excitation(y).view(B, C, 1) # Excitation: [B, C, 1]
        return x * y.expand_as(x) # Scale (Re-calibrate)

class MultiScaleTrendSE(nn.Module):
    """ë©€í‹°ìŠ¤ì¼€ì¼ Conv1dì™€ SE ë¸”ë¡ì„ ê²°í•©í•œ ì¶”ì„¸ ëª¨ë¸."""
    def __init__(self, enc_in, seq_len, pred_len, kernel_sizes=[3, 7, 15, 25], reduction_ratio=4):
        super().__init__()
        self.enc_in = enc_in
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.kernel_sizes = kernel_sizes
        
        # 1. ì—¬ëŸ¬ ê°œì˜ Conv1d ë ˆì´ì–´ë¥¼ ë‹´ì„ ModuleList
        self.trend_convs = nn.ModuleList()
        for k in self.kernel_sizes:
            self.trend_convs.append(
                nn.Conv1d(in_channels=self.enc_in, out_channels=self.enc_in, kernel_size=k, padding='same')
            )
            
        # 2. í•©ì³ì§„ ì¶”ì„¸ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•  SE ë¸”ë¡
        num_combined_channels = self.enc_in * len(self.kernel_sizes)
        self.se_block = SEBlock(num_combined_channels, reduction_ratio=reduction_ratio)
            
        # 3. SE ë¸”ë¡ì„ ê±°ì¹œ ì¶”ì„¸ë“¤ì„ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë˜ëŒë¦´ Linear ë ˆì´ì–´
        self.projection = nn.Linear(num_combined_channels, self.enc_in)

    def forward(self, x):
        # x shape: [B, L, C] (Batch, Seq_Len, Channels/Features)
        B, L, C = x.shape
        
        # Conv1dì— ë§ê²Œ shape ë³€ê²½: [B, C, L]
        x_transposed = x.permute(0, 2, 1)
        
        # ê° Conv ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì˜ ì¶”ì„¸ ì¶”ì¶œ
        trend_outputs = []
        for conv in self.trend_convs:
            trend_outputs.append(conv(x_transposed))
            
        # ëª¨ë“  ì¶”ì„¸ ê²°ê³¼ë¥¼ ì±„ë„(C) ê¸°ì¤€ìœ¼ë¡œ í•©ì¹˜ê¸°
        concatenated_trends = torch.cat(trend_outputs, dim=1) # shape: [B, C * num_kernels, L]
        
        # SE ë¸”ë¡ì„ í†µí•´ ì±„ë„ë³„(ìŠ¤ì¼€ì¼ë³„) ì¤‘ìš”ë„ ì ìš©
        recalibrated_trends = self.se_block(concatenated_trends)
        
        # Linear ë ˆì´ì–´ì— ë§ê²Œ shape ë³µì›: [B, L, C * num_kernels]
        recalibrated_trends = recalibrated_trends.permute(0, 2, 1)
        
        # ìµœì¢… ì¶”ì„¸ ê³„ì‚°
        final_trend = self.projection(recalibrated_trends) # shape: [B, L, C]
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì„¸ ë¶„ë¦¬
        detrended_x = x - final_trend
        
        # ë¯¸ë˜ ì¶”ì„¸ ì˜ˆì¸¡ (ê°„ë‹¨í•œ ë²„ì „: ë§ˆì§€ë§‰ ì¶”ì„¸ ê°’ì„ ë°˜ë³µ)
        # ë” ì •êµí•˜ê²Œ ë§Œë“¤ë ¤ë©´ ì´ ë¶€ë¶„ì— MLP ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        future_trend = final_trend[:, -1:, :].repeat(1, self.pred_len, 1)
        
        return detrended_x, future_trend