import torch
import torch.nn as nn
from .TSF_TSM_layers import *

# nflows ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from nflows.flows.base import Flow
from nflows.distributions.normal import StandardNormal
from nflows.transforms.base import CompositeTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform
from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform
from nflows.transforms.permutations import RandomPermutation
from nflows.transforms.normalization import BatchNorm
from nflows.nn.nets import ResidualNet

import logging

class PredictionHead(nn.Module):
    """
    ì¸ì½”ë”ê°€ ë§Œë“  ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ë°›ì•„, 
    ë¯¸ë˜ ì˜ˆì¸¡ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ ì„ í˜• í—¤ë“œ.
    """
    def __init__(self, configs):
        super().__init__()
        self.pred_len = configs.pred_len
        self.enc_in = configs.enc_in
        self.fc1 = nn.Linear(configs.d_model, configs.pred_len * configs.enc_in * 4)
        self.fc2 = nn.Linear(configs.pred_len * configs.enc_in * 4, configs.pred_len * configs.enc_in * 2)
        self.head = nn.Linear(configs.pred_len * configs.enc_in * 2, configs.pred_len * configs.enc_in)

    def forward(self, summary_context):
        x = self.fc1(summary_context)
        x = self.fc2(x)
        output = self.head(x).view(-1, self.pred_len, self.enc_in)
        return output

class ProbabilisticResidualModel(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.c_in = configs.enc_in
        self.pred_len = configs.pred_len
        context_dim = configs.d_model
        
        self.window_size = configs.moving_avg
        self.stride = configs.stride

        feature_dim = self.c_in * self.window_size
        
        self.flow_head = create_conditional_nsf_flow(
            feature_dim=feature_dim, 
            context_dim=context_dim,
            num_layers=configs.flow_layers, 
            hidden_features=configs.hidden_features, 
            num_bins=configs.num_bins
        )

    def forward(self, summary_context, y):
        # y shape: [B, pred_len, c_in]
        B, L, C = y.shape
        
        # --- ğŸ’¡ [í•µì‹¬] for ë£¨í”„ë¥¼ unfold ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´ ---
        # 1. yë¥¼ [B, C, L] í˜•íƒœë¡œ ë³€í™˜
        y_transposed = y.permute(0, 2, 1)
        
        # 2. unfoldë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìœˆë„ìš°ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
        # ê²°ê³¼ shape: [B, C, num_windows, window_size]
        y_windows = y_transposed.unfold(dimension=2, size=self.window_size, step=self.stride)
        
        # 3. Flowì— ì…ë ¥í•˜ê¸° ìœ„í•´ shape ì¬ì •ë ¬ ë° flatten
        # [B, C, num_windows, window_size] -> [B, num_windows, C, window_size]
        y_windows = y_windows.permute(0, 2, 1, 3)
        num_windows = y_windows.shape[1]
        
        # [B, num_windows, C, window_size] -> [B * num_windows, C * window_size]
        y_windows_flat = y_windows.reshape(B * num_windows, -1)
        
        # 4. summary_contextë¥¼ ê° ìœˆë„ìš°ì— ë§ê²Œ í™•ì¥
        # [B, d_model] -> [B, num_windows, d_model] -> [B * num_windows, d_model]
        expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
        expanded_context = expanded_context.reshape(B * num_windows, -1)
        
        # 5. ëª¨ë“  ìœˆë„ìš°ì— ëŒ€í•´ Flowë¥¼ ë³‘ë ¬ë¡œ í•œ ë²ˆì— ì‹¤í–‰
        log_prob_windows = self.flow_head.log_prob(y_windows_flat, context=expanded_context)
        
        # 6. ì „ì²´ í‰ê·  Loss ê³„ì‚°
        total_log_prob = log_prob_windows.mean()
        
        return total_log_prob

    def sample(self, summary_context, num_samples=1):
        self.eval()
        with torch.no_grad():
            B = summary_context.size(0)
            num_windows = (self.pred_len - self.window_size) // self.stride + 1
            
            # 1. ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª¨ë“  ìœˆë„ìš°ì— ë§ê²Œ í™•ì¥
            expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
            expanded_context = expanded_context.reshape(B * num_windows, -1)
            
            # 2. ëª¨ë“  ìœˆë„ìš°ì— ëŒ€í•œ ìƒ˜í”Œì„ ë³‘ë ¬ë¡œ í•œ ë²ˆì— ìƒì„±
            # sample_windows_flat shape: [num_samples, B * num_windows, window_size * c_in]
            sample_windows_flat = self.flow_head.sample(num_samples, context=expanded_context)
            
            # 3. ìœˆë„ìš° shapeìœ¼ë¡œ ë³µì›
            # [num_samples, B, num_windows, window_size, c_in]
            sample_windows = sample_windows_flat.view(num_samples, B, num_windows, self.window_size, self.c_in)
            
            # 4. ê²¹ì¹˜ëŠ”(Overlapping) ìœˆë„ìš°ë“¤ì„ í‰ê· ë‚´ì–´ ìµœì¢… ì‹œí€€ìŠ¤ ìƒì„± (Overlap-add ë°©ì‹)
            final_samples = torch.zeros(num_samples, B, self.pred_len, self.c_in, device=summary_context.device)
            counts = torch.zeros(B, self.pred_len, self.c_in, device=summary_context.device)
            
            for i in range(num_windows):
                start_idx = i * self.stride
                end_idx = start_idx + self.window_size
                final_samples[:, :, start_idx:end_idx, :] += sample_windows[:, :, i, :, :]
                counts[:, start_idx:end_idx, :] += 1
            
            final_samples = final_samples / counts.unsqueeze(0)

        return final_samples