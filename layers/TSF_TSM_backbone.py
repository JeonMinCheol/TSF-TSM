import torch
import math
import torch.nn as nn
from .TSF_TSM_layers import *
import logging

class MeanPredictionHead(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.pred_len = configs.pred_len
        self.enc_in = configs.enc_in
        d_model = configs.d_model
        d_ff = configs.d_ff

        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_ff),
            nn.Dropout(configs.dropout),
            nn.Linear(d_ff, self.pred_len * self.enc_in)
        )

    def forward(self, summary_context):
        # summary_context shape: [B, d_model]
        output = self.head(summary_context) # [B, pred_len * enc_in]
        
        # ìµœì¢… shapeìœ¼ë¡œ ì¬êµ¬ì„±: [B, pred_len, enc_in]
        output = output.view(-1, self.pred_len, self.enc_in)
        return output

class ProbabilisticResidualModel(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.c_in = configs.enc_in
        self.pred_len = configs.pred_len
        context_dim = configs.d_model
        
        self.window_size = configs.patch_len
        self.stride = configs.stride

        feature_dim = self.c_in * self.window_size
        
        self.flow_head = create_conditional_nsf_flow(
            feature_dim=feature_dim, 
            context_dim=context_dim,
            num_layers=configs.flow_layers, 
            hidden_features=configs.hidden_features, 
            num_bins=configs.num_bins
        )

    def forward(self, summary_context, y):
        # y shape: [B, pred_len, c_in]
        B, L, C = y.shape
        
        # --- ğŸ’¡ [í•µì‹¬] for ë£¨í”„ë¥¼ unfold ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´ ---
        # 1. yë¥¼ [B, C, L] í˜•íƒœë¡œ ë³€í™˜
        y_transposed = y.permute(0, 2, 1)
        
        # 2. unfoldë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìœˆë„ìš°ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
        # ê²°ê³¼ shape: [B, C, num_windows, window_size]
        y_windows = y_transposed.unfold(dimension=2, size=self.window_size, step=self.stride)
        
        # 3. Flowì— ì…ë ¥í•˜ê¸° ìœ„í•´ shape ì¬ì •ë ¬ ë° flatten
        # [B, C, num_windows, window_size] -> [B, num_windows, C, window_size]
        y_windows = y_windows.permute(0, 2, 1, 3)
        num_windows = y_windows.shape[1]
        
        # [B, num_windows, C, window_size] -> [B * num_windows, C * window_size]
        y_windows_flat = y_windows.reshape(B * num_windows, -1)
        
        # 4. summary_contextë¥¼ ê° ìœˆë„ìš°ì— ë§ê²Œ í™•ì¥
        # [B, d_model] -> [B, num_windows, d_model] -> [B * num_windows, d_model]
        expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
        expanded_context = expanded_context.reshape(B * num_windows, -1)
        
        # 5. ëª¨ë“  ìœˆë„ìš°ì— ëŒ€í•´ Flowë¥¼ ë³‘ë ¬ë¡œ í•œ ë²ˆì— ì‹¤í–‰
        log_prob_windows = self.flow_head.log_prob(y_windows_flat, context=expanded_context)
        
        # 6. ì „ì²´ í‰ê·  Loss ê³„ì‚°
        nll_loss = -log_prob_windows.mean()
        
        return nll_loss

    def sample(self, summary_context, num_samples=5):
        self.eval()
        with torch.no_grad():
            B = summary_context.size(0)
            num_windows = (self.pred_len - self.window_size) // self.stride + 1
            
            # 1. ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª¨ë“  ìœˆë„ìš°ì— ë§ê²Œ í™•ì¥
            expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
            expanded_context = expanded_context.reshape(B * num_windows, -1)
            
            # 2. ëª¨ë“  ìœˆë„ìš°ì— ëŒ€í•œ ìƒ˜í”Œì„ ë³‘ë ¬ë¡œ í•œ ë²ˆì— ìƒì„±
            # sample_windows_flat shape: [num_samples, B * num_windows, window_size * c_in]
            sample_windows_flat = self.flow_head.sample(num_samples, context=expanded_context)
            
            # 3. ìœˆë„ìš° shapeìœ¼ë¡œ ë³µì›
            # [num_samples, B, num_windows, window_size, c_in]
            sample_windows = sample_windows_flat.view(num_samples, B, num_windows, self.window_size, self.c_in)
            
            # 4. ê²¹ì¹˜ëŠ”(Overlapping) ìœˆë„ìš°ë“¤ì„ í‰ê· ë‚´ì–´ ìµœì¢… ì‹œí€€ìŠ¤ ìƒì„± (Overlap-add ë°©ì‹)
            final_samples = torch.zeros(num_samples, B, self.pred_len, self.c_in, device=summary_context.device)
            counts = torch.zeros(B, self.pred_len, self.c_in, device=summary_context.device)
            
            for i in range(num_windows):
                start_idx = i * self.stride
                end_idx = start_idx + self.window_size
                final_samples[:, :, start_idx:end_idx, :] += sample_windows[:, :, i, :, :]
                counts[:, start_idx:end_idx, :] += 1
            
            final_samples = final_samples / counts.unsqueeze(0).clamp(min=1.0)

        return final_samples

class GaussianResidualModel(nn.Module):
    def __init__(self, configs):
        super().__init__()
        d_model = configs.d_model
        c_in = configs.enc_in

        # context â†’ [mean, log_var]
        self.fc = nn.Linear(d_model, 2 * c_in)

    def forward(self, context, y=None):
        params = self.fc(context)  # [B, 2*C]
        mean, log_var = params.chunk(2, dim=-1)

        # ë¶„ì‚° ë²”ìœ„ í™•ì¥
        log_var = torch.clamp(log_var, min=-10.0, max=5.0)
        std = torch.exp(0.5 * log_var)

        if y is not None:
            B, L, C = y.shape
            mean = mean.unsqueeze(1).expand(B, L, C)
            std = std.unsqueeze(1).expand(B, L, C)
            log_var = log_var.unsqueeze(1).expand(B, L, C)

            # Gaussian NLL (ì•ˆì •í™” ë²„ì „)
            nll = 0.5 * (
                torch.clamp(log_var, min=-10.0, max=10.0) +
                ((y - mean) ** 2) / (torch.clamp(std**2, min=1e-6)) +
                math.log(2 * math.pi)
            )
            return nll.mean()
        else:
            return mean, std

    def sample(self, context, num_samples=1, length=1):
        """
        context: [B, d_model]
        return: [num_samples, B, length, C]
        """
        params = self.fc(context)
        mean, log_var = params.chunk(2, dim=-1)
        # log_var = torch.clamp(log_var, min=-5.0, max=2.0)
        std = torch.exp(0.5 * log_var)

        B, C = mean.shape
        mean = mean.unsqueeze(1).unsqueeze(0).expand(num_samples, B, length, C)
        std = std.unsqueeze(1).unsqueeze(0).expand(num_samples, B, length, C)

        eps = torch.randn_like(std)
        samples = mean + eps * std
        return samples