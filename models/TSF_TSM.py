import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.signal import periodogram
import os, sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# nflows ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from nflows.flows.base import Flow
from nflows.distributions.normal import StandardNormal
from nflows.transforms.base import CompositeTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform
from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform
from nflows.transforms.permutations import RandomPermutation
from nflows.transforms.normalization import BatchNorm
from nflows.nn.nets import ResidualNet

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from layers.TST_TSM_backbone import *
from layers.TST_TSM_layers import *

torch.manual_seed(42)
np.random.seed(42)

import logging
logging.basicConfig(level=logging.DEBUG)

class Model(nn.Module):
    # __init__ì—ì„œ mean, scale ê´€ë ¨ ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤.
    def __init__(self, configs, mean, scale): # mean, scale ì¸ì ì œê±°
        super().__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.eval_samples = configs.eval_samples
        self.enc_in = configs.enc_in
        self.d_model = configs.d_model
        self.feature_dim = configs.d_ff
        self.n_heads = configs.n_heads
        self.chunk_size = configs.chunk_size
        self.patch_len = configs.patch_len
        self.stride = configs.stride
        self.flow_layers = configs.flow_layers
        self.hidden_features = configs.hidden_features
        self.num_bins = configs.num_bins

        self.shared_encoder = SharedEncoder(self.seq_len, enc_in=self.enc_in, feature_dim=self.feature_dim, n_heads=self.n_heads, patch_len=self.patch_len, stride=self.stride)
        # self.detrender = LearnableTrend(self.seq_len, self.pred_len, self.enc_in)
        self.detrender = MultiScaleTrendSE(self.enc_in, self.seq_len, self.pred_len)
        # self.deterministic_model = DeterministicModel(self.shared_encoder, self.pred_len, self.enc_in)
        self.deterministic_model = Decoder(self.enc_in, self.d_model, self.n_heads, self.feature_dim, 4, 0.1)
        self.residual_model = ProbabilisticResidualModel(self.shared_encoder, self.pred_len, self.enc_in, self.chunk_size, self.flow_layers, self.hidden_features, self.num_bins)
        
        self.loss_fn_mse = nn.MSELoss()

        self.register_buffer('mean', torch.tensor(mean, dtype=torch.float32))
        self.register_buffer('scale', torch.tensor(scale, dtype=torch.float32))

    def _preprocess(self, x_raw, y_raw=None):
        """[2ë‹¨ê³„ ìˆ˜ì •] ì›ë³¸ ë°ì´í„°ë¥¼ ë°›ì•„ 'ì¶”ì„¸ ë¶„ë¦¬ -> ìŠ¤ì¼€ì¼ë§'ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # 1. ì¶”ì„¸ ë¶„ë¦¬ ë¨¼ì € ìˆ˜í–‰
        x_detrended_raw, _ = self.detrender(x_raw)
        
        # 2. ì¶”ì„¸ê°€ ì œê±°ëœ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§
        x_scaled = (x_detrended_raw - self.mean) / self.scale

        if y_raw is not None:
            # í•™ìŠµ ì‹œì—ëŠ” xì™€ yë¥¼ í•©ì³ì„œ ì „ì²´ íŠ¸ë Œë“œë¥¼ ì¼ê´€ë˜ê²Œ ê³„ì‚°
            full_raw = torch.cat([x_raw, y_raw], dim=1)
            full_detrended_raw, _ = self.detrender(full_raw)
            y_detrended_raw = full_detrended_raw[:, self.seq_len:]
            
            # ì¶”ì„¸ ì œê±°ëœ yë¥¼ ìŠ¤ì¼€ì¼ë§
            y_scaled = (y_detrended_raw - self.mean) / self.scale
            return x_scaled, y_scaled
        
        return x_scaled, None

    # forward í•¨ìˆ˜ì—ì„œ _preprocess ëŒ€ì‹  _detrendë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    def forward(self, x_raw, y_raw):
        """[3ë‹¨ê³„ ìˆ˜ì •] í•œ ë²ˆì˜ forward í˜¸ì¶œë¡œ ëª¨ë“  ì†ì‹¤ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        # 1. ì „ì²˜ë¦¬ (ì¶”ì„¸ ë¶„ë¦¬ -> ìŠ¤ì¼€ì¼ë§)
        x_scaled, y_scaled = self._preprocess(x_raw, y_raw)
        
        # 2. ê³µìœ  ì¸ì½”ë” ê³„ì‚° (í•œ ë²ˆë§Œ ì‹¤í–‰)
        encoder_outputs, summary_context = self.shared_encoder(x_scaled)
        
        # 3. Deterministic ë¶€ë¶„ ê³„ì‚°
        y_target = y_scaled[:, -self.pred_len:, :]
        decoder_input = torch.zeros_like(y_target).to(x_raw.device)

        mean_pred = self.deterministic_model(decoder_input, encoder_outputs) 

        deter_loss = self.loss_fn_mse(mean_pred, y_target)
        
        # 4. Residual ë¶€ë¶„ ê³„ì‚°
        # gradientê°€ íë¥´ì§€ ì•Šë„ë¡ .detach() ì‚¬ìš©
        residuals = y_target - mean_pred.detach()
        
        log_prob, ar_pred = self.residual_model(summary_context, y=residuals)
        
        nll_loss = -log_prob.mean()
        # ar_predëŠ” residualì— ëŒ€í•œ ì˜ˆì¸¡ì´ë¯€ë¡œ, ì‹¤ì œ residualê³¼ ë¹„êµ
        mse_loss = self.loss_fn_mse(ar_pred, residuals)
            
        return deter_loss, nll_loss, mse_loss
    
    def sample(self, x_raw):
        x_scaled, _ = self._preprocess(x_raw)
        
        # ğŸ’¡ [ì¶”ê°€] shared_encoderë¥¼ í†µê³¼ì‹œì¼œ ë””ì½”ë”ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ìŠµë‹ˆë‹¤.
        encoder_outputs, summary_context = self.shared_encoder(x_scaled)
        
        # --- Deterministic Decoder ì˜ˆì¸¡ ---
        # ğŸ’¡ [ì¶”ê°€] forwardì™€ ë™ì¼í•˜ê²Œ ë””ì½”ë” ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
        y_target_placeholder = torch.zeros(x_raw.size(0), self.pred_len, self.enc_in).to(x_raw.device)
        decoder_input = torch.zeros_like(y_target_placeholder).to(x_raw.device)
        
        # ğŸ’¡ [ìˆ˜ì •] Decoderë¥¼ ì˜¬ë°”ë¥¸ ì¸ìë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
        mean_pred_scaled = self.deterministic_model(decoder_input, encoder_outputs)

        # --- Probabilistic Residual ì˜ˆì¸¡ ---
        # ğŸ’¡ [ìˆ˜ì •] x_scaledë¥¼ ë‹¤ì‹œ ë„£ëŠ” ëŒ€ì‹ , ê³„ì‚°í•´ ë‘” summary_contextë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
        #    ì´ë¥¼ ìœ„í•´ TST_TSM_backbone.pyì˜ ProbabilisticResidualModel.sample ì¸ìë¥¼ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        #    (sample(self, x) -> sample(self, summary_context))
        #    ìš°ì„ ì€ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë‘ë˜, ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ì ì„ ì¸ì§€í•©ë‹ˆë‹¤.
        residual_samples_scaled = self.residual_model.sample(x_scaled, num_samples=self.eval_samples)
        
        # --- ê²°ê³¼ ì·¨í•© ---
        final_samples_scaled = mean_pred_scaled.unsqueeze(0) + residual_samples_scaled
        mean_final_pred_scaled = final_samples_scaled.mean(dim=0)

        # 3. ì—­ë³€í™˜ (un-scale -> ì¶”ì„¸ ë³µì›)
        # 3a. Un-scaling
        mean_final_pred_detrended = (mean_final_pred_scaled * self.scale) + self.mean
        
        # 3b. ë¯¸ë˜ ì¶”ì„¸ ê³„ì‚° ë° ë”í•˜ê¸°
        _, future_trend = self.detrender(x_raw)
        final_pred_raw = mean_final_pred_detrended + future_trend # future_trendëŠ” ì´ë¯¸ [B, pred_len, C] shapeì¼ ê²ƒ
        
        return final_pred_raw